Now let me provide the final summary:

---

**Status: DONE**

## Summary of Changes

Created a comprehensive regression test suite for `Billing\UsageTracker` to validate that usage tracking remains accurate after the OpenAI refactor.

### New File Created
- **`tests/Unit/Billing/UsageTrackerTest.php`** - 20 unit tests covering:

### Test Coverage

**Usage Metadata Accuracy (Non-Streaming):**
- Verifies OpenAI usage metadata is correctly extracted from non-streaming responses
- Validates fallback estimation (tiktoken) is used when OpenAI doesn't return usage

**Usage Metadata Accuracy (Streaming):**
- Verifies usage is extracted from the final usage chunk in streaming responses
- Validates fallback estimation for streams without usage chunks
- Confirms `stream_options.include_usage = true` is sent in streaming requests

**Tool Call Usage Estimation:**
- Tests fallback estimation includes token counts for tool calls
- Validates streaming tool calls with usage fallback

**Model Tracking:**
- Verifies model from response takes precedence over configured model
- Tests model extraction from streaming responses
- Validates configured model is used when response omits it

**Intent Type Tracking:**
- Confirms intent type is properly passed through to UsageTracker

**Retry with Usage Tracking:**
- Validates usage metadata is correct after retries (uses successful response data)

**Edge Cases:**
- Zero token usage handling
- Large token counts
- Partial usage objects

**Regression Tests:**
- `test_regression_stream_usage_extraction` - Critical test ensuring streaming responses with usage chunks correctly extract OpenAI usage data instead of falling back to estimation
- `test_regression_combined_content_and_tool_calls_estimation` - Ensures combined content + tool_calls estimation works when OpenAI doesn't return usage

## How to Verify

```bash
# Run the UsageTracker tests
./vendor/bin/phpunit tests/Unit/Billing/UsageTrackerTest.php --testdox

# Run all tests to ensure no regressions
./vendor/bin/phpunit --testdox
```

Expected: All 280 tests pass (20 new UsageTracker tests + 260 existing tests)

## Key Findings

The analysis confirmed that the OpenAI refactor correctly maintains usage tracking:

1. **Streaming Responses**: The `parse_stream_response()` method properly extracts usage from the final chunk when `stream_options.include_usage = true` is set
2. **Fallback Estimation**: When OpenAI doesn't return usage data, `TokenCounter` is used to estimate tokens via tiktoken (or character approximation)
3. **UsageTracker Integration**: `UsageTracker::log_usage()` is called with correct model, input_tokens, output_tokens, and intent_type after successful responses
4. **Model Normalization**: Both `Model::normalize()` and `UsageTracker::normalize_model()` handle model variants correctly

## Risks / Follow-ups

1. **Integration Testing**: The tests mock HTTP responses. Consider adding an integration test that validates against actual OpenAI API responses (with VCR recording) for complete E2E validation.

2. **Database Testing**: The tests verify metadata flow but don't test actual database writes. Consider adding tests for `UsageTracker::log_usage()` and `UsageTracker::get_usage_summary()` with a test database.

3. **No Breaking Changes**: The existing OpenAIClient tests continue to pass, confirming backward compatibility.
